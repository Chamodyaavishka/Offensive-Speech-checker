#### Project Overview
This project develops a machine learning model to identify offensive speech from text inputs. The primary objective is to classify phrases into categories such as "Offensive Speech" and "No Hate and Offensive Speech," leveraging natural language processing techniques.

#### Data Preparation and Preprocessing
The dataset used in this project comprises various tweets which were labeled according to the presence of offensive content. The preprocessing steps included:

Text Cleaning: Removal of URLs, special characters, and stopwords. Text was also converted to lowercase to maintain consistency.
Tokenization and Vectorization: The cleaned text was transformed into a numerical format using the CountVectorizer from scikit-learn, which converts the text data into a matrix of token counts.

#### Model Building and Evaluation
A decision tree classifier was employed for the task due to its simplicity and interpretability. The model was trained on the preprocessed data, and its performance was evaluated using several metrics:

##### Accuracy: Measures the overall correctness of the model.
##### Precision and Recall: Provide insight into the balance between the modelâ€™s sensitivity and specificity.
##### F1 Score: Harmonic mean of precision and recall, used as a single metric to gauge the model's robustness.
The model showed promising results in distinguishing between offensive and non-offensive speech, as validated through metrics like accuracy and F1 score.

#### Implementation
The trained model was integrated into a user interface using Gradio, allowing users to input speech which is then converted to text, analyzed, and classified in real-time. This interface enhances the accessibility of the model, facilitating its practical use in monitoring and filtering offensive content in digital communications.

#### Conclusion
This project underscores the potential of machine learning in enhancing digital communication environments by automating the detection of offensive speech. Future improvements might include optimizing the model with more sophisticated algorithms and expanding the dataset to encompass a broader spectrum of linguistic nuances.